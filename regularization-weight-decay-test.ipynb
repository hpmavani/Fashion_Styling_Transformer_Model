{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11721486,"sourceType":"datasetVersion","datasetId":7358155}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:17:54.738123Z","iopub.execute_input":"2025-05-07T17:17:54.738936Z","iopub.status.idle":"2025-05-07T17:17:56.684780Z","shell.execute_reply.started":"2025-05-07T17:17:54.738917Z","shell.execute_reply":"2025-05-07T17:17:56.684120Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fashion-ai-dataset/train (1).txt\n/kaggle/input/fashion-ai-dataset/valid (1).txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"with open(\"../input/fashion-ai-dataset/train (1).txt\") as f: \n    train_outfits = f.readlines()\nwith open(\"../input/fashion-ai-dataset/valid (1).txt\") as f: \n    valid_outfits = f.readlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:37.550291Z","iopub.execute_input":"2025-05-07T17:18:37.550945Z","iopub.status.idle":"2025-05-07T17:18:37.576910Z","shell.execute_reply.started":"2025-05-07T17:18:37.550922Z","shell.execute_reply":"2025-05-07T17:18:37.576209Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_outfits[0:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Create Tokenizer \nimport torch\nfrom transformers import GPT2LMHeadModel, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:18:43.006608Z","iopub.execute_input":"2025-05-07T17:18:43.007439Z","iopub.status.idle":"2025-05-07T17:19:09.045390Z","shell.execute_reply.started":"2025-05-07T17:18:43.007405Z","shell.execute_reply":"2025-05-07T17:19:09.044746Z"}},"outputs":[{"name":"stderr","text":"2025-05-07 17:18:57.328587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746638337.562949      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746638337.633035      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:59.882632Z","iopub.execute_input":"2025-05-07T18:03:59.883477Z","iopub.status.idle":"2025-05-07T18:04:00.799254Z","shell.execute_reply.started":"2025-05-07T18:03:59.883450Z","shell.execute_reply":"2025-05-07T18:04:00.798682Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"num_added_toks = tokenizer.add_special_tokens({'additional_special_tokens': ['[SEP]', '[OUTFIT_END]']})\nprint(num_added_toks)\nmodel.resize_token_embeddings(len(tokenizer))\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:02.085812Z","iopub.execute_input":"2025-05-07T18:04:02.086086Z","iopub.status.idle":"2025-05-07T18:04:02.122603Z","shell.execute_reply.started":"2025-05-07T18:04:02.086067Z","shell.execute_reply":"2025-05-07T18:04:02.121793Z"}},"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"for i in train_outfits: \n    encoding = tokenizer.encode(i)\n    if len(encoding) >= 1024:\n        print(f\"{i} has an encoding larger than 1024.\")\n\nfor i in valid_outfits: \n    encoding = tokenizer.encode(i)\n    if len(encoding) >= 1024:\n        print(f\"{i} has an encoding larger than 1024.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:03.663947Z","iopub.execute_input":"2025-05-07T18:04:03.664902Z","iopub.status.idle":"2025-05-07T18:04:06.762750Z","shell.execute_reply.started":"2025-05-07T18:04:03.664871Z","shell.execute_reply":"2025-05-07T18:04:06.762180Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"tokenizer.tokenize(\"mock embroidery turtle neck[SEP]black leather jacket[SEP]gold earrings[OUTFIT_END]\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T17:33:34.317161Z","iopub.execute_input":"2025-05-07T17:33:34.317621Z","iopub.status.idle":"2025-05-07T17:33:34.322996Z","shell.execute_reply.started":"2025-05-07T17:33:34.317597Z","shell.execute_reply":"2025-05-07T17:33:34.322289Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['m',\n 'ock',\n 'Ġembro',\n 'ider',\n 'y',\n 'Ġturtle',\n 'Ġneck',\n '[SEP]',\n 'black',\n 'Ġleather',\n 'Ġjacket',\n '[SEP]',\n 'gold',\n 'Ġear',\n 'rings',\n '[OUTFIT_END]',\n 'Ċ']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# 17,316 is a little large; we need to set up a data loader so we can feed in batches \n# Our learning task is next-token prediction, so this is a little different than the classic use case\n\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler\nimport torch\n\nclass OutfitDataset(Dataset): \n    def __init__ (self, data, tokenizer): \n        self.data = data\n        self.input_ids = []\n        self.attn_masks = []\n        for outfit in data: \n            encodings = tokenizer.encode_plus(outfit,\n                                             truncation=True,\n                                             padding='max_length',\n                                             return_tensors='pt'\n                                             )\n            self.input_ids.append(torch.squeeze(encodings['input_ids'], 0))\n            self.attn_masks.append(torch.squeeze(encodings['attention_mask'], 0))\n\n    def __len__(self): \n        return len(self.data)\n    def __getitem__(self, idx): \n        return self.input_ids[idx], self.attn_masks[idx]\n        \ntrain_dataset = OutfitDataset(train_outfits, tokenizer=tokenizer)\nvalid_dataset = OutfitDataset(valid_outfits, tokenizer=tokenizer)\nprint(f\"input_ids: {train_dataset[0][0]} attn_masks: {train_dataset[0][1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:10.332829Z","iopub.execute_input":"2025-05-07T18:04:10.333126Z","iopub.status.idle":"2025-05-07T18:04:25.763217Z","shell.execute_reply.started":"2025-05-07T18:04:10.333088Z","shell.execute_reply":"2025-05-07T18:04:25.762598Z"}},"outputs":[{"name":"stdout","text":"input_ids: tensor([   76,   735,  7393,  ..., 50256, 50256, 50256]) attn_masks: tensor([1, 1, 1,  ..., 0, 0, 0])\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Small Subset Testing\n\nfrom torch.utils.data import random_split\n\nsmall_train_dataset_size = 1000\nsmall_val_dataset_size = 400\n\nsmall_train_dataset, _ = random_split(train_dataset, [small_train_dataset_size, len(train_dataset) - small_train_dataset_size])\nsmall_val_dataset, _ = random_split(valid_dataset, [small_val_dataset_size, len(valid_dataset) - small_val_dataset_size])\n\nsubset_train_dl = DataLoader(\n    small_train_dataset, \n    sampler = RandomSampler(small_train_dataset), \n    batch_size = 4,\n)\n\nsubset_val_dl = DataLoader(\n    small_val_dataset, \n    sampler = RandomSampler(small_val_dataset), \n    batch_size = 4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:42.002467Z","iopub.execute_input":"2025-05-07T18:04:42.003016Z","iopub.status.idle":"2025-05-07T18:04:42.008974Z","shell.execute_reply.started":"2025-05-07T18:04:42.002995Z","shell.execute_reply":"2025-05-07T18:04:42.008430Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import gc \ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:44.188944Z","iopub.execute_input":"2025-05-07T18:04:44.189575Z","iopub.status.idle":"2025-05-07T18:04:44.913381Z","shell.execute_reply.started":"2025-05-07T18:04:44.189551Z","shell.execute_reply":"2025-05-07T18:04:44.912861Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from torch.optim import SGD\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom transformers import get_scheduler\nfrom torch.amp import autocast, GradScaler\nimport time\n\ndef overfit_experiment(model, train_dl, valid_dl, train_len, valid_len, learning_rate, epochs, dropout_rate, weight_decay): \n    model.config.dropout = dropout_rate\n    model = model.to(device)\n    model.train() \n    optimizer = torch.optim.SGD(model.parameters(), lr=3e-5, momentum=0.9, weight_decay=weight_decay)\n    \n    total_steps = epochs * len(train_dl)\n    scheduler = get_scheduler(\n        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps\n    )\n    scaler = GradScaler(\"cuda\") \n    \n    for epoch in range(epochs): \n        start_time = time.time()\n        total_train_loss = 0.0 \n        total_perplexity = 0.0\n        \n        for step, batch in enumerate(train_dl):\n            b_input_ids = batch[0].to(device)\n            b_labels = batch[0].to(device)\n            b_attention = batch[1].to(device)\n            \n            #forward pass\n            with autocast(\"cuda\"): \n                outputs = model(input_ids=b_input_ids, labels=b_labels, attention_mask=b_attention) #the model will shift labels to the left for next token prediction\n                loss = outputs.loss\n                total_train_loss += loss.item() \n                perplexity = torch.exp(loss)\n                total_perplexity += perplexity.item()\n                \n                print(f\"Step {step}: Loss: {loss}, Perplexity: {perplexity}\")\n\n            \n            #back_prop\n            optimizer.zero_grad()\n            \n            scaler.scale(loss).backward() #compute gradients\n\n            scaler.unscale_(optimizer)\n            clip_grad_norm_(model.parameters(), max_norm=1.0) #gradient clipping\n            \n            scaler.step(optimizer) #update weights \n            scaler.update()\n            \n            scheduler.step()\n\n        #Each Epoch\n        avg_train_loss = total_train_loss/len(train_dl)\n        avg_train_perp = total_perplexity/len(train_dl)\n\n        model.eval()\n        total_valid_loss = 0\n        total_perp_val = 0\n        with torch.no_grad(): \n            for step, batch in enumerate(valid_dl): \n                inputs = batch[0].to(device)\n                labels = batch[0].to(device)\n                attention = batch[1].to(device)\n                \n                outputs = model(inputs, labels=labels, attention_mask=attention)\n                total_valid_loss += outputs.loss.item()\n                total_perp_val += torch.exp(outputs.loss).item()\n                \n        avg_valid_loss = total_valid_loss/len(valid_dl)\n        avg_val_perp = total_perp_val/len(valid_dl)\n        \n        overfit_gap = avg_valid_loss - avg_train_loss\n        end_time = time.time() \n        epoch_duration = end_time - start_time\n        print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss: .4f}, Valid loss: {avg_valid_loss: .4f}, Gap: {overfit_gap: .4f}, Train Perp: {avg_train_perp: .4f}, Valid Perp: {avg_val_perp : .4f}\")\n        print(f\"Duration: {epoch_duration}\")\n        return avg_train_loss, avg_valid_loss, overfit_gap, avg_train_perp, avg_val_perp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:45.923479Z","iopub.execute_input":"2025-05-07T18:04:45.923757Z","iopub.status.idle":"2025-05-07T18:04:45.933780Z","shell.execute_reply.started":"2025-05-07T18:04:45.923711Z","shell.execute_reply":"2025-05-07T18:04:45.933254Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import pandas as pd\ndropouts = [0.1, 0.2, 0.3]\nweight_decays = [0.0, 0.01]\n\nresults = [] \n\nfor d in dropouts:\n    for w in weight_decays: \n        train_loss, val_loss, gap, train_perp, val_perp  = overfit_experiment(\n            model,\n            subset_train_dl,\n            subset_val_dl,\n            len(small_train_dataset), \n            len(small_val_dataset), \n            learning_rate=3e-5, \n            epochs=2, \n            dropout_rate=d,\n            weight_decay=w\n        )\n        results.append({\"dropout\": d, \"weight_decay\": w, \"train_loss\": train_loss, \"val_loss\": val_loss, \"overfit_gap\": gap, \"train_perp\": train_perp, \"val_perp\": val_perp})\n\nresult_df = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:04:53.354699Z","iopub.execute_input":"2025-05-07T18:04:53.355408Z","iopub.status.idle":"2025-05-07T18:16:00.903720Z","shell.execute_reply.started":"2025-05-07T18:04:53.355389Z","shell.execute_reply":"2025-05-07T18:16:00.902667Z"}},"outputs":[{"name":"stdout","text":"Step 0: Loss: 8.720152854919434, Perplexity: 6125.115234375\nStep 1: Loss: 8.434131622314453, Perplexity: 4601.47265625\nStep 2: Loss: 8.421244621276855, Perplexity: 4542.5537109375\nStep 3: Loss: 8.694890975952148, Perplexity: 5972.32177734375\nStep 4: Loss: 8.18997573852539, Perplexity: 3604.634765625\nStep 5: Loss: 8.783745765686035, Perplexity: 6527.28125\nStep 6: Loss: 8.70047664642334, Perplexity: 6005.7744140625\nStep 7: Loss: 8.388779640197754, Perplexity: 4397.44775390625\nStep 8: Loss: 9.123404502868652, Perplexity: 9167.3583984375\nStep 9: Loss: 7.813678741455078, Perplexity: 2474.215576171875\nStep 10: Loss: 8.497917175292969, Perplexity: 4904.54296875\nStep 11: Loss: 8.451754570007324, Perplexity: 4683.28271484375\nStep 12: Loss: 8.46178150177002, Perplexity: 4730.47802734375\nStep 13: Loss: 8.358341217041016, Perplexity: 4265.61328125\nStep 14: Loss: 8.398307800292969, Perplexity: 4439.5478515625\nStep 15: Loss: 7.951657772064209, Perplexity: 2840.279541015625\nStep 16: Loss: 8.686711311340332, Perplexity: 5923.66943359375\nStep 17: Loss: 8.052650451660156, Perplexity: 3142.112060546875\nStep 18: Loss: 8.060592651367188, Perplexity: 3167.166748046875\nStep 19: Loss: 8.471884727478027, Perplexity: 4778.51318359375\nStep 20: Loss: 7.796835422515869, Perplexity: 2432.890869140625\nStep 21: Loss: 7.873075008392334, Perplexity: 2625.627197265625\nStep 22: Loss: 8.384138107299805, Perplexity: 4377.083984375\nStep 23: Loss: 7.798979759216309, Perplexity: 2438.11328125\nStep 24: Loss: 7.891445159912109, Perplexity: 2674.305908203125\nStep 25: Loss: 8.452897071838379, Perplexity: 4688.63623046875\nStep 26: Loss: 7.866567611694336, Perplexity: 2608.596435546875\nStep 27: Loss: 8.141495704650879, Perplexity: 3434.050537109375\nStep 28: Loss: 7.679586887359619, Perplexity: 2163.7255859375\nStep 29: Loss: 7.665722846984863, Perplexity: 2133.934814453125\nStep 30: Loss: 7.594494819641113, Perplexity: 1987.2257080078125\nStep 31: Loss: 7.252054214477539, Perplexity: 1411.0003662109375\nStep 32: Loss: 7.088619232177734, Perplexity: 1198.252197265625\nStep 33: Loss: 7.27373743057251, Perplexity: 1441.9295654296875\nStep 34: Loss: 7.450895309448242, Perplexity: 1721.403564453125\nStep 35: Loss: 7.687965393066406, Perplexity: 2181.930908203125\nStep 36: Loss: 7.1883440017700195, Perplexity: 1323.9090576171875\nStep 37: Loss: 7.2935099601745605, Perplexity: 1470.723876953125\nStep 38: Loss: 6.970072269439697, Perplexity: 1064.2996826171875\nStep 39: Loss: 7.265771865844727, Perplexity: 1430.4893798828125\nStep 40: Loss: 7.383375644683838, Perplexity: 1609.0120849609375\nStep 41: Loss: 6.96026086807251, Perplexity: 1053.908447265625\nStep 42: Loss: 7.320565223693848, Perplexity: 1511.0577392578125\nStep 43: Loss: 7.1078925132751465, Perplexity: 1221.5704345703125\nStep 44: Loss: 7.447662353515625, Perplexity: 1715.8472900390625\nStep 45: Loss: 6.376142978668213, Perplexity: 587.65673828125\nStep 46: Loss: 6.240645408630371, Perplexity: 513.1896362304688\nStep 47: Loss: 6.858557224273682, Perplexity: 951.9926147460938\nStep 48: Loss: 6.500551223754883, Perplexity: 665.5083618164062\nStep 49: Loss: 6.843775272369385, Perplexity: 938.023681640625\nStep 50: Loss: 6.694174766540527, Perplexity: 807.6871337890625\nStep 51: Loss: 6.154053211212158, Perplexity: 470.6210632324219\nStep 52: Loss: 6.18757963180542, Perplexity: 486.6667785644531\nStep 53: Loss: 6.1903252601623535, Perplexity: 488.00482177734375\nStep 54: Loss: 6.413692951202393, Perplexity: 610.1427612304688\nStep 55: Loss: 6.067729473114014, Perplexity: 431.69940185546875\nStep 56: Loss: 5.957417964935303, Perplexity: 386.610595703125\nStep 57: Loss: 5.253632545471191, Perplexity: 191.259765625\nStep 58: Loss: 6.196433067321777, Perplexity: 490.9945983886719\nStep 59: Loss: 6.491338729858398, Perplexity: 659.4055786132812\nStep 60: Loss: 6.0430731773376465, Perplexity: 421.1854248046875\nStep 61: Loss: 6.01630163192749, Perplexity: 410.0592346191406\nStep 62: Loss: 5.679658889770508, Perplexity: 292.8495178222656\nStep 63: Loss: 5.6174540519714355, Perplexity: 275.1878967285156\nStep 64: Loss: 5.966293811798096, Perplexity: 390.057373046875\nStep 65: Loss: 5.312791347503662, Perplexity: 202.9158477783203\nStep 66: Loss: 6.150196552276611, Perplexity: 468.8095397949219\nStep 67: Loss: 5.908653259277344, Perplexity: 368.2099609375\nStep 68: Loss: 5.090738296508789, Perplexity: 162.50979614257812\nStep 69: Loss: 4.88879919052124, Perplexity: 132.7940216064453\nStep 70: Loss: 5.137901306152344, Perplexity: 170.3578643798828\nStep 71: Loss: 5.0536417961120605, Perplexity: 156.59170532226562\nStep 72: Loss: 4.858117580413818, Perplexity: 128.78155517578125\nStep 73: Loss: 5.692397117614746, Perplexity: 296.603759765625\nStep 74: Loss: 4.905017852783203, Perplexity: 134.96531677246094\nStep 75: Loss: 5.494081497192383, Perplexity: 243.24798583984375\nStep 76: Loss: 5.233788967132568, Perplexity: 187.50189208984375\nStep 77: Loss: 5.177225112915039, Perplexity: 177.19044494628906\nStep 78: Loss: 5.366644382476807, Perplexity: 214.14306640625\nStep 79: Loss: 4.263331890106201, Perplexity: 71.04630279541016\nStep 80: Loss: 4.994648456573486, Perplexity: 147.6210479736328\nStep 81: Loss: 4.573240756988525, Perplexity: 96.85749053955078\nStep 82: Loss: 4.678101062774658, Perplexity: 107.56561279296875\nStep 83: Loss: 4.660834789276123, Perplexity: 105.72431182861328\nStep 84: Loss: 4.462789058685303, Perplexity: 86.72906494140625\nStep 85: Loss: 4.825278282165527, Perplexity: 124.62114715576172\nStep 86: Loss: 3.967975616455078, Perplexity: 52.87738037109375\nStep 87: Loss: 4.5148186683654785, Perplexity: 91.36100006103516\nStep 88: Loss: 4.251805305480957, Perplexity: 70.23208618164062\nStep 89: Loss: 4.5701904296875, Perplexity: 96.5625\nStep 90: Loss: 3.8434407711029053, Perplexity: 46.68583297729492\nStep 91: Loss: 3.5818886756896973, Perplexity: 35.94136047363281\nStep 92: Loss: 4.345699787139893, Perplexity: 77.14600372314453\nStep 93: Loss: 4.195580959320068, Perplexity: 66.39229583740234\nStep 94: Loss: 3.74460506439209, Perplexity: 42.292301177978516\nStep 95: Loss: 4.075835227966309, Perplexity: 58.899654388427734\nStep 96: Loss: 3.811459541320801, Perplexity: 45.21638488769531\nStep 97: Loss: 4.261713981628418, Perplexity: 70.93145751953125\nStep 98: Loss: 3.7120473384857178, Perplexity: 40.93753433227539\nStep 99: Loss: 3.5569114685058594, Perplexity: 35.05476379394531\nStep 100: Loss: 3.5537662506103516, Perplexity: 34.94468307495117\nStep 101: Loss: 4.341709613800049, Perplexity: 76.83879089355469\nStep 102: Loss: 3.864445924758911, Perplexity: 47.67684555053711\nStep 103: Loss: 3.8759405612945557, Perplexity: 48.2280387878418\nStep 104: Loss: 3.455190420150757, Perplexity: 31.664318084716797\nStep 105: Loss: 3.506776809692383, Perplexity: 33.340633392333984\nStep 106: Loss: 4.241980075836182, Perplexity: 69.54541778564453\nStep 107: Loss: 3.025394916534424, Perplexity: 20.602140426635742\nStep 108: Loss: 3.7191848754882812, Perplexity: 41.230770111083984\nStep 109: Loss: 3.4270758628845215, Perplexity: 30.786487579345703\nStep 110: Loss: 3.1313791275024414, Perplexity: 22.905548095703125\nStep 111: Loss: 3.0850939750671387, Perplexity: 21.869522094726562\nStep 112: Loss: 3.4100377559661865, Perplexity: 30.266386032104492\nStep 113: Loss: 3.4808478355407715, Perplexity: 32.48725128173828\nStep 114: Loss: 3.048996686935425, Perplexity: 21.09416961669922\nStep 115: Loss: 2.90865159034729, Perplexity: 18.332063674926758\nStep 116: Loss: 3.060713529586792, Perplexity: 21.34278106689453\nStep 117: Loss: 2.822395086288452, Perplexity: 16.817079544067383\nStep 118: Loss: 2.629857301712036, Perplexity: 13.871790885925293\nStep 119: Loss: 2.866549253463745, Perplexity: 17.576261520385742\nStep 120: Loss: 2.42395281791687, Perplexity: 11.290400505065918\nStep 121: Loss: 4.439209461212158, Perplexity: 84.70794677734375\nStep 122: Loss: 3.5406339168548584, Perplexity: 34.48877716064453\nStep 123: Loss: 3.3639979362487793, Perplexity: 28.904518127441406\nStep 124: Loss: 3.1827945709228516, Perplexity: 24.11404800415039\nStep 125: Loss: 2.7055397033691406, Perplexity: 14.962389945983887\nStep 126: Loss: 3.104274034500122, Perplexity: 22.29302978515625\nStep 127: Loss: 2.471604347229004, Perplexity: 11.841429710388184\nStep 128: Loss: 2.8027327060699463, Perplexity: 16.489646911621094\nStep 129: Loss: 2.3257694244384766, Perplexity: 10.234552383422852\nStep 130: Loss: 3.1504716873168945, Perplexity: 23.347074508666992\nStep 131: Loss: 1.9568672180175781, Perplexity: 7.077121734619141\nStep 132: Loss: 2.6360573768615723, Perplexity: 13.958064079284668\nStep 133: Loss: 2.596618175506592, Perplexity: 13.418283462524414\nStep 134: Loss: 2.4247829914093018, Perplexity: 11.299777030944824\nStep 135: Loss: 2.7944247722625732, Perplexity: 16.353219985961914\nStep 136: Loss: 2.698989152908325, Perplexity: 14.86469841003418\nStep 137: Loss: 2.4796793460845947, Perplexity: 11.9374361038208\nStep 138: Loss: 2.67120361328125, Perplexity: 14.45736026763916\nStep 139: Loss: 2.1690173149108887, Perplexity: 8.74968147277832\nStep 140: Loss: 3.163297414779663, Perplexity: 23.648447036743164\nStep 141: Loss: 2.294058084487915, Perplexity: 9.915093421936035\nStep 142: Loss: 2.0419156551361084, Perplexity: 7.705356121063232\nStep 143: Loss: 2.295408010482788, Perplexity: 9.928486824035645\nStep 144: Loss: 2.1662378311157227, Perplexity: 8.725396156311035\nStep 145: Loss: 2.1037261486053467, Perplexity: 8.1966552734375\nStep 146: Loss: 2.11574387550354, Perplexity: 8.295754432678223\nStep 147: Loss: 2.575788974761963, Perplexity: 13.141681671142578\nStep 148: Loss: 1.7862489223480225, Perplexity: 5.96702766418457\nStep 149: Loss: 1.8726871013641357, Perplexity: 6.5057549476623535\nStep 150: Loss: 2.676534652709961, Perplexity: 14.534638404846191\nStep 151: Loss: 1.8962231874465942, Perplexity: 6.660690784454346\nStep 152: Loss: 2.1618916988372803, Perplexity: 8.687556266784668\nStep 153: Loss: 2.207540988922119, Perplexity: 9.093328475952148\nStep 154: Loss: 2.471334934234619, Perplexity: 11.838239669799805\nStep 155: Loss: 1.8222163915634155, Perplexity: 6.185553073883057\nStep 156: Loss: 1.9513585567474365, Perplexity: 7.038243293762207\nStep 157: Loss: 2.811953544616699, Perplexity: 16.642398834228516\nStep 158: Loss: 1.307163119316101, Perplexity: 3.695674419403076\nStep 159: Loss: 1.8449597358703613, Perplexity: 6.327845573425293\nStep 160: Loss: 2.0640923976898193, Perplexity: 7.878144264221191\nStep 161: Loss: 2.662351131439209, Perplexity: 14.329941749572754\nStep 162: Loss: 2.12044358253479, Perplexity: 8.334834098815918\nStep 163: Loss: 1.8706779479980469, Perplexity: 6.492697238922119\nStep 164: Loss: 1.8980658054351807, Perplexity: 6.672975063323975\nStep 165: Loss: 1.9206410646438599, Perplexity: 6.825332164764404\nStep 166: Loss: 2.597160816192627, Perplexity: 13.425566673278809\nStep 167: Loss: 1.7446811199188232, Perplexity: 5.724075794219971\nStep 168: Loss: 1.570859432220459, Perplexity: 4.810781002044678\nStep 169: Loss: 2.0265088081359863, Perplexity: 7.587550163269043\nStep 170: Loss: 1.6049755811691284, Perplexity: 4.977737903594971\nStep 171: Loss: 1.6266148090362549, Perplexity: 5.086626052856445\nStep 172: Loss: 1.5863534212112427, Perplexity: 4.885899543762207\nStep 173: Loss: 1.6172239780426025, Perplexity: 5.0390825271606445\nStep 174: Loss: 1.3920984268188477, Perplexity: 4.023283958435059\nStep 175: Loss: 1.316963791847229, Perplexity: 3.7320728302001953\nStep 176: Loss: 1.6112534999847412, Perplexity: 5.0090861320495605\nStep 177: Loss: 2.0244805812835693, Perplexity: 7.572176456451416\nStep 178: Loss: 1.2580684423446655, Perplexity: 3.51861834526062\nStep 179: Loss: 1.5733314752578735, Perplexity: 4.822688102722168\nStep 180: Loss: 1.9401500225067139, Perplexity: 6.9597954750061035\nStep 181: Loss: 1.5494321584701538, Perplexity: 4.708795547485352\nStep 182: Loss: 1.8967348337173462, Perplexity: 6.664099216461182\nStep 183: Loss: 1.6788122653961182, Perplexity: 5.359187126159668\nStep 184: Loss: 1.4151579141616821, Perplexity: 4.117136478424072\nStep 185: Loss: 1.2815656661987305, Perplexity: 3.6022753715515137\nStep 186: Loss: 1.081684947013855, Perplexity: 2.949645519256592\nStep 187: Loss: 1.3610055446624756, Perplexity: 3.9001128673553467\nStep 188: Loss: 1.5066711902618408, Perplexity: 4.511687278747559\nStep 189: Loss: 1.1370508670806885, Perplexity: 3.117560625076294\nStep 190: Loss: 1.0648483037948608, Perplexity: 2.9003989696502686\nStep 191: Loss: 1.486088752746582, Perplexity: 4.419775009155273\nStep 192: Loss: 1.1706136465072632, Perplexity: 3.223970413208008\nStep 193: Loss: 1.2312870025634766, Perplexity: 3.425635576248169\nStep 194: Loss: 1.550258755683899, Perplexity: 4.712689399719238\nStep 195: Loss: 1.3853486776351929, Perplexity: 3.9962191581726074\nStep 196: Loss: 1.0985430479049683, Perplexity: 2.9997923374176025\nStep 197: Loss: 1.4223315715789795, Perplexity: 4.146777629852295\nStep 198: Loss: 1.1016390323638916, Perplexity: 3.009093999862671\nStep 199: Loss: 1.3572582006454468, Perplexity: 3.8855252265930176\nStep 200: Loss: 1.2120786905288696, Perplexity: 3.3604626655578613\nStep 201: Loss: 1.0705127716064453, Perplexity: 2.916874885559082\nStep 202: Loss: 1.7334998846054077, Perplexity: 5.660430431365967\nStep 203: Loss: 1.1450040340423584, Perplexity: 3.142453908920288\nStep 204: Loss: 1.0780792236328125, Perplexity: 2.9390289783477783\nStep 205: Loss: 1.2877777814865112, Perplexity: 3.624722719192505\nStep 206: Loss: 1.0467334985733032, Perplexity: 2.848331928253174\nStep 207: Loss: 1.7247859239578247, Perplexity: 5.611319541931152\nStep 208: Loss: 1.3136714696884155, Perplexity: 3.719805955886841\nStep 209: Loss: 1.4197853803634644, Perplexity: 4.136232852935791\nStep 210: Loss: 1.3088932037353516, Perplexity: 3.7020740509033203\nStep 211: Loss: 1.1605607271194458, Perplexity: 3.1917223930358887\nStep 212: Loss: 0.9880468845367432, Perplexity: 2.685983419418335\nStep 213: Loss: 1.138389229774475, Perplexity: 3.1217358112335205\nStep 214: Loss: 1.0121620893478394, Perplexity: 2.7515437602996826\nStep 215: Loss: 2.4854307174682617, Perplexity: 12.006290435791016\nStep 216: Loss: 1.070065975189209, Perplexity: 2.915571689605713\nStep 217: Loss: 0.9662331342697144, Perplexity: 2.628026247024536\nStep 218: Loss: 0.9310168027877808, Perplexity: 2.5370876789093018\nStep 219: Loss: 1.6681913137435913, Perplexity: 5.302568435668945\nStep 220: Loss: 0.9949299693107605, Perplexity: 2.7045350074768066\nStep 221: Loss: 0.9179762005805969, Perplexity: 2.5042171478271484\nStep 222: Loss: 1.585378646850586, Perplexity: 4.881139278411865\nStep 223: Loss: 1.66804838180542, Perplexity: 5.301810264587402\nStep 224: Loss: 1.0705139636993408, Perplexity: 2.9168782234191895\nStep 225: Loss: 0.9630239605903625, Perplexity: 2.6196060180664062\nStep 226: Loss: 2.1109561920166016, Perplexity: 8.256132125854492\nStep 227: Loss: 1.1238737106323242, Perplexity: 3.076749801635742\nStep 228: Loss: 2.279972791671753, Perplexity: 9.776413917541504\nStep 229: Loss: 0.9090331196784973, Perplexity: 2.481921672821045\nStep 230: Loss: 1.1871639490127563, Perplexity: 3.2777721881866455\nStep 231: Loss: 0.9606844782829285, Perplexity: 2.6134846210479736\nStep 232: Loss: 1.6537556648254395, Perplexity: 5.226572513580322\nStep 233: Loss: 0.9593796133995056, Perplexity: 2.610076904296875\nStep 234: Loss: 0.9132300615310669, Perplexity: 2.4923598766326904\nStep 235: Loss: 0.856442391872406, Perplexity: 2.3547685146331787\nStep 236: Loss: 0.900772750377655, Perplexity: 2.4615046977996826\nStep 237: Loss: 0.9842925071716309, Perplexity: 2.6759181022644043\nStep 238: Loss: 0.8568739891052246, Perplexity: 2.3557851314544678\nStep 239: Loss: 0.8900570273399353, Perplexity: 2.4352686405181885\nStep 240: Loss: 1.17139732837677, Perplexity: 3.2264978885650635\nStep 241: Loss: 0.8233556747436523, Perplexity: 2.2781317234039307\nStep 242: Loss: 0.9528186321258545, Perplexity: 2.593008041381836\nStep 243: Loss: 0.9804770350456238, Perplexity: 2.6657276153564453\nStep 244: Loss: 0.9478496313095093, Perplexity: 2.580155611038208\nStep 245: Loss: 0.989109218120575, Perplexity: 2.688838243484497\nStep 246: Loss: 1.6584906578063965, Perplexity: 5.251378536224365\nStep 247: Loss: 0.8098272681236267, Perplexity: 2.2475197315216064\nStep 248: Loss: 0.8146045207977295, Perplexity: 2.258282423019409\nStep 249: Loss: 0.9829884171485901, Perplexity: 2.6724307537078857\nEpoch 1: Train Loss:  3.7394, Valid loss:  0.9363, Gap: -2.8031, Train Perp:  659.7185, Valid Perp:  2.5818\nDuration: 202.56354308128357\nStep 0: Loss: 0.6861838698387146, Perplexity: 1.986121654510498\nStep 1: Loss: 0.8990591764450073, Perplexity: 2.4572904109954834\nStep 2: Loss: 0.8778778314590454, Perplexity: 2.4057888984680176\nStep 3: Loss: 0.8270447850227356, Perplexity: 2.2865514755249023\nStep 4: Loss: 1.1233536005020142, Perplexity: 3.0751497745513916\nStep 5: Loss: 1.8047443628311157, Perplexity: 6.078417778015137\nStep 6: Loss: 0.7491527795791626, Perplexity: 2.1152071952819824\nStep 7: Loss: 1.5674885511398315, Perplexity: 4.794591903686523\nStep 8: Loss: 0.8812482357025146, Perplexity: 2.4139108657836914\nStep 9: Loss: 0.8708271980285645, Perplexity: 2.388885974884033\nStep 10: Loss: 1.3284345865249634, Perplexity: 3.7751293182373047\nStep 11: Loss: 0.8367418646812439, Perplexity: 2.3088324069976807\nStep 12: Loss: 0.753594160079956, Perplexity: 2.1246225833892822\nStep 13: Loss: 0.8009535074234009, Perplexity: 2.227663993835449\nStep 14: Loss: 0.6830408573150635, Perplexity: 1.979889154434204\nStep 15: Loss: 1.8382257223129272, Perplexity: 6.28537654876709\nStep 16: Loss: 0.8781513571739197, Perplexity: 2.406446933746338\nStep 17: Loss: 0.9409058094024658, Perplexity: 2.5623013973236084\nStep 18: Loss: 0.7066650986671448, Perplexity: 2.027219295501709\nStep 19: Loss: 1.3436968326568604, Perplexity: 3.833188056945801\nStep 20: Loss: 0.9009025692939758, Perplexity: 2.4618241786956787\nStep 21: Loss: 1.4142143726348877, Perplexity: 4.113253593444824\nStep 22: Loss: 0.8622884154319763, Perplexity: 2.368574857711792\nStep 23: Loss: 0.8103858828544617, Perplexity: 2.2487754821777344\nStep 24: Loss: 0.8000267744064331, Perplexity: 2.225600481033325\nStep 25: Loss: 1.0514390468597412, Perplexity: 2.8617665767669678\nStep 26: Loss: 0.6832475662231445, Perplexity: 1.980298399925232\nStep 27: Loss: 0.6709434986114502, Perplexity: 1.9560819864273071\nStep 28: Loss: 0.754237174987793, Perplexity: 2.1259891986846924\nStep 29: Loss: 0.7037378549575806, Perplexity: 2.021293878555298\nStep 30: Loss: 0.826339066028595, Perplexity: 2.284938335418701\nStep 31: Loss: 0.8329224586486816, Perplexity: 2.3000307083129883\nStep 32: Loss: 0.799726665019989, Perplexity: 2.2249326705932617\nStep 33: Loss: 0.8157053589820862, Perplexity: 2.260769844055176\nStep 34: Loss: 0.6763126850128174, Perplexity: 1.9666128158569336\nStep 35: Loss: 0.7971736788749695, Perplexity: 2.219259738922119\nStep 36: Loss: 0.6523502469062805, Perplexity: 1.9200481176376343\nStep 37: Loss: 0.8153643012046814, Perplexity: 2.2599987983703613\nStep 38: Loss: 0.7090111970901489, Perplexity: 2.0319809913635254\nStep 39: Loss: 0.6527559161186218, Perplexity: 1.9208271503448486\nStep 40: Loss: 0.7366794943809509, Perplexity: 2.0889875888824463\nStep 41: Loss: 1.436521053314209, Perplexity: 4.206037521362305\nStep 42: Loss: 1.2409836053848267, Perplexity: 3.4590139389038086\nStep 43: Loss: 0.8129104971885681, Perplexity: 2.254460096359253\nStep 44: Loss: 0.7207307815551758, Perplexity: 2.0559351444244385\nStep 45: Loss: 0.7707021832466125, Perplexity: 2.161283493041992\nStep 46: Loss: 0.7207533121109009, Perplexity: 2.055981397628784\nStep 47: Loss: 0.6887537837028503, Perplexity: 1.9912325143814087\nStep 48: Loss: 1.6666054725646973, Perplexity: 5.294166088104248\nStep 49: Loss: 0.6671331524848938, Perplexity: 1.9486428499221802\nStep 50: Loss: 0.6617185473442078, Perplexity: 1.9381202459335327\nStep 51: Loss: 0.6187515258789062, Perplexity: 1.8566086292266846\nStep 52: Loss: 0.6214322447776794, Perplexity: 1.861592411994934\nStep 53: Loss: 1.8847546577453613, Perplexity: 6.5847392082214355\nStep 54: Loss: 0.7004048824310303, Perplexity: 2.014568328857422\nStep 55: Loss: 1.2723737955093384, Perplexity: 3.5693154335021973\nStep 56: Loss: 1.2358810901641846, Perplexity: 3.4414093494415283\nStep 57: Loss: 0.53140789270401, Perplexity: 1.7013258934020996\nStep 58: Loss: 1.3580087423324585, Perplexity: 3.8884425163269043\nStep 59: Loss: 0.6743302345275879, Perplexity: 1.962717890739441\nStep 60: Loss: 1.045904278755188, Perplexity: 2.84597110748291\nStep 61: Loss: 0.9321283102035522, Perplexity: 2.5399091243743896\nStep 62: Loss: 0.7041741609573364, Perplexity: 2.0221760272979736\nStep 63: Loss: 0.6242320537567139, Perplexity: 1.8668118715286255\nStep 64: Loss: 1.2311489582061768, Perplexity: 3.4251625537872314\nStep 65: Loss: 0.5405330061912537, Perplexity: 1.7169218063354492\nStep 66: Loss: 0.7094734907150269, Perplexity: 2.0329205989837646\nStep 67: Loss: 0.5888310074806213, Perplexity: 1.8018808364868164\nStep 68: Loss: 1.2291580438613892, Perplexity: 3.4183502197265625\nStep 69: Loss: 0.5429654121398926, Perplexity: 1.7211030721664429\nStep 70: Loss: 0.6171374320983887, Perplexity: 1.853614330291748\nStep 71: Loss: 0.6447715163230896, Perplexity: 1.9055516719818115\nStep 72: Loss: 0.7548086643218994, Perplexity: 2.127204418182373\nStep 73: Loss: 0.6470695734024048, Perplexity: 1.909935712814331\nStep 74: Loss: 0.7020249366760254, Perplexity: 2.0178346633911133\nStep 75: Loss: 0.618104100227356, Perplexity: 1.8554069995880127\nStep 76: Loss: 1.1490596532821655, Perplexity: 3.155224561691284\nStep 77: Loss: 0.621954619884491, Perplexity: 1.862565040588379\nStep 78: Loss: 1.3961334228515625, Perplexity: 4.03955078125\nStep 79: Loss: 0.6881017684936523, Perplexity: 1.9899345636367798\nStep 80: Loss: 0.5738744735717773, Perplexity: 1.775131344795227\nStep 81: Loss: 0.5290775299072266, Perplexity: 1.6973657608032227\nStep 82: Loss: 1.214129090309143, Perplexity: 3.3673598766326904\nStep 83: Loss: 0.6169611215591431, Perplexity: 1.8532874584197998\nStep 84: Loss: 0.5967707633972168, Perplexity: 1.8162442445755005\nStep 85: Loss: 0.6378543972969055, Perplexity: 1.8924161195755005\nStep 86: Loss: 0.6951192617416382, Perplexity: 2.0039479732513428\nStep 87: Loss: 0.498615026473999, Perplexity: 1.6464394330978394\nStep 88: Loss: 1.1973735094070435, Perplexity: 3.311408042907715\nStep 89: Loss: 0.725720226764679, Perplexity: 2.066218614578247\nStep 90: Loss: 1.5304023027420044, Perplexity: 4.620035171508789\nStep 91: Loss: 0.5480411052703857, Perplexity: 1.7298610210418701\nStep 92: Loss: 0.650864839553833, Perplexity: 1.9171981811523438\nStep 93: Loss: 0.6103217601776123, Perplexity: 1.8410236835479736\nStep 94: Loss: 0.7116479277610779, Perplexity: 2.0373458862304688\nStep 95: Loss: 0.6913256049156189, Perplexity: 1.9963603019714355\nStep 96: Loss: 0.5412277579307556, Perplexity: 1.718114972114563\nStep 97: Loss: 0.6150504946708679, Perplexity: 1.84975004196167\nStep 98: Loss: 0.5656118988990784, Perplexity: 1.7605247497558594\nStep 99: Loss: 1.4307950735092163, Perplexity: 4.182023048400879\nStep 100: Loss: 0.7425607442855835, Perplexity: 2.1013095378875732\nStep 101: Loss: 0.5845724940299988, Perplexity: 1.7942237854003906\nStep 102: Loss: 0.7323377728462219, Perplexity: 2.07993745803833\nStep 103: Loss: 0.6529302597045898, Perplexity: 1.9211620092391968\nStep 104: Loss: 0.6392792463302612, Perplexity: 1.8951144218444824\nStep 105: Loss: 0.7390785217285156, Perplexity: 2.0940051078796387\nStep 106: Loss: 0.9485663175582886, Perplexity: 2.582005262374878\nStep 107: Loss: 0.5522472262382507, Perplexity: 1.7371524572372437\nStep 108: Loss: 0.5629382133483887, Perplexity: 1.7558238506317139\nStep 109: Loss: 0.5784711241722107, Perplexity: 1.7833099365234375\nStep 110: Loss: 0.572883129119873, Perplexity: 1.7733725309371948\nStep 111: Loss: 0.61671382188797, Perplexity: 1.8528293371200562\nStep 112: Loss: 1.072066068649292, Perplexity: 2.9214091300964355\nStep 113: Loss: 0.9755634069442749, Perplexity: 2.6526613235473633\nStep 114: Loss: 0.7010430693626404, Perplexity: 2.0158543586730957\nStep 115: Loss: 0.5841949582099915, Perplexity: 1.7935465574264526\nStep 116: Loss: 0.8454688191413879, Perplexity: 2.3290696144104004\nStep 117: Loss: 0.9436478018760681, Perplexity: 2.5693366527557373\nStep 118: Loss: 0.5721240639686584, Perplexity: 1.7720268964767456\nStep 119: Loss: 0.5663279891014099, Perplexity: 1.761785864830017\nStep 120: Loss: 0.5784484148025513, Perplexity: 1.7832694053649902\nStep 121: Loss: 0.6664608716964722, Perplexity: 1.9473333358764648\nStep 122: Loss: 0.568518877029419, Perplexity: 1.7656500339508057\nStep 123: Loss: 0.5636868476867676, Perplexity: 1.7571388483047485\nStep 124: Loss: 0.6255151033401489, Perplexity: 1.869208574295044\nStep 125: Loss: 0.5882886648178101, Perplexity: 1.8009039163589478\nStep 126: Loss: 0.5958514213562012, Perplexity: 1.8145753145217896\nStep 127: Loss: 0.5194376111030579, Perplexity: 1.6810818910598755\nStep 128: Loss: 0.6132886409759521, Perplexity: 1.8464938402175903\nStep 129: Loss: 0.5394083857536316, Perplexity: 1.7149920463562012\nStep 130: Loss: 0.6164519786834717, Perplexity: 1.852344274520874\nStep 131: Loss: 0.5239071846008301, Perplexity: 1.688612461090088\nStep 132: Loss: 0.528699517250061, Perplexity: 1.6967244148254395\nStep 133: Loss: 0.539301872253418, Perplexity: 1.7148091793060303\nStep 134: Loss: 0.6184878349304199, Perplexity: 1.856119155883789\nStep 135: Loss: 0.5532147288322449, Perplexity: 1.738834023475647\nStep 136: Loss: 0.6166582703590393, Perplexity: 1.8527263402938843\nStep 137: Loss: 0.6018533706665039, Perplexity: 1.8254989385604858\nStep 138: Loss: 0.6074902415275574, Perplexity: 1.8358181715011597\nStep 139: Loss: 0.8592702150344849, Perplexity: 2.361436605453491\nStep 140: Loss: 0.6117374897003174, Perplexity: 1.8436318635940552\nStep 141: Loss: 0.5578643679618835, Perplexity: 1.7469377517700195\nStep 142: Loss: 0.6917132139205933, Perplexity: 1.9971340894699097\nStep 143: Loss: 0.6317493915557861, Perplexity: 1.880898118019104\nStep 144: Loss: 0.6328205466270447, Perplexity: 1.8829139471054077\nStep 145: Loss: 0.5927637219429016, Perplexity: 1.8089810609817505\nStep 146: Loss: 0.57948899269104, Perplexity: 1.785125970840454\nStep 147: Loss: 0.78355473279953, Perplexity: 2.1892404556274414\nStep 148: Loss: 0.5128097534179688, Perplexity: 1.6699767112731934\nStep 149: Loss: 0.7325491905212402, Perplexity: 2.0803771018981934\nStep 150: Loss: 0.6020944118499756, Perplexity: 1.8259391784667969\nStep 151: Loss: 0.6447542309761047, Perplexity: 1.9055185317993164\nStep 152: Loss: 0.6297252774238586, Perplexity: 1.8770948648452759\nStep 153: Loss: 0.5751870274543762, Perplexity: 1.7774629592895508\nStep 154: Loss: 0.6499453783035278, Perplexity: 1.9154362678527832\nStep 155: Loss: 0.6548961997032166, Perplexity: 1.9249428510665894\nStep 156: Loss: 0.5813265442848206, Perplexity: 1.7884093523025513\nStep 157: Loss: 0.4791496694087982, Perplexity: 1.6147007942199707\nStep 158: Loss: 0.5758748650550842, Perplexity: 1.7786859273910522\nStep 159: Loss: 0.9809427261352539, Perplexity: 2.6669692993164062\nStep 160: Loss: 0.6792250871658325, Perplexity: 1.9723488092422485\nStep 161: Loss: 1.335369348526001, Perplexity: 3.8013997077941895\nStep 162: Loss: 0.6585119962692261, Perplexity: 1.931915521621704\nStep 163: Loss: 0.39724060893058777, Perplexity: 1.4877138137817383\nStep 164: Loss: 0.5162121057510376, Perplexity: 1.6756683588027954\nStep 165: Loss: 0.5514214634895325, Perplexity: 1.7357184886932373\nStep 166: Loss: 0.6722548007965088, Perplexity: 1.958648681640625\nStep 167: Loss: 0.567685604095459, Perplexity: 1.7641792297363281\nStep 168: Loss: 0.4771106243133545, Perplexity: 1.611411690711975\nStep 169: Loss: 0.6179496049880981, Perplexity: 1.8551204204559326\nStep 170: Loss: 0.6050135493278503, Perplexity: 1.8312771320343018\nStep 171: Loss: 0.6225822567939758, Perplexity: 1.863734483718872\nStep 172: Loss: 0.7090205550193787, Perplexity: 2.0319998264312744\nStep 173: Loss: 0.41334858536720276, Perplexity: 1.5118719339370728\nStep 174: Loss: 0.6575834155082703, Perplexity: 1.9301224946975708\nStep 175: Loss: 0.5012688040733337, Perplexity: 1.6508145332336426\nStep 176: Loss: 0.4736778438091278, Perplexity: 1.6058895587921143\nStep 177: Loss: 0.5173811316490173, Perplexity: 1.6776283979415894\nStep 178: Loss: 0.7281954884529114, Perplexity: 2.0713393688201904\nStep 179: Loss: 0.4849720895290375, Perplexity: 1.6241296529769897\nStep 180: Loss: 0.5102452039718628, Perplexity: 1.6656996011734009\nStep 181: Loss: 0.6176533102989197, Perplexity: 1.854570746421814\nStep 182: Loss: 0.5395276546478271, Perplexity: 1.7151964902877808\nStep 183: Loss: 0.45615479350090027, Perplexity: 1.577994704246521\nStep 184: Loss: 0.6073125004768372, Perplexity: 1.8354918956756592\nStep 185: Loss: 0.6116513013839722, Perplexity: 1.843472957611084\nStep 186: Loss: 0.5280635356903076, Perplexity: 1.6956455707550049\nStep 187: Loss: 0.5750182867050171, Perplexity: 1.777163028717041\nStep 188: Loss: 0.46571534872055054, Perplexity: 1.5931533575057983\nStep 189: Loss: 0.5327988862991333, Perplexity: 1.7036941051483154\nStep 190: Loss: 0.6431519389152527, Perplexity: 1.9024678468704224\nStep 191: Loss: 0.6557815074920654, Perplexity: 1.926647663116455\nStep 192: Loss: 1.3677088022232056, Perplexity: 3.926344156265259\nStep 193: Loss: 0.5472179055213928, Perplexity: 1.7284376621246338\nStep 194: Loss: 0.6259010434150696, Perplexity: 1.8699300289154053\nStep 195: Loss: 0.6099385619163513, Perplexity: 1.8403183221817017\nStep 196: Loss: 0.5027126669883728, Perplexity: 1.6531996726989746\nStep 197: Loss: 0.6090373396873474, Perplexity: 1.838660478591919\nStep 198: Loss: 0.6054612398147583, Perplexity: 1.832097053527832\nStep 199: Loss: 0.49757829308509827, Perplexity: 1.6447334289550781\nStep 200: Loss: 0.4915134012699127, Perplexity: 1.6347883939743042\nStep 201: Loss: 0.6238595843315125, Perplexity: 1.8661165237426758\nStep 202: Loss: 1.0530643463134766, Perplexity: 2.8664214611053467\nStep 203: Loss: 0.6089909672737122, Perplexity: 1.8385752439498901\nStep 204: Loss: 0.4876304268836975, Perplexity: 1.6284528970718384\nStep 205: Loss: 0.556134819984436, Perplexity: 1.7439188957214355\nStep 206: Loss: 0.5398179888725281, Perplexity: 1.7156946659088135\nStep 207: Loss: 0.6214529275894165, Perplexity: 1.8616307973861694\nStep 208: Loss: 0.5168794989585876, Perplexity: 1.67678701877594\nStep 209: Loss: 0.5899062752723694, Perplexity: 1.8038192987442017\nStep 210: Loss: 0.6062018275260925, Perplexity: 1.8334542512893677\nStep 211: Loss: 0.5423398613929749, Perplexity: 1.7200267314910889\nStep 212: Loss: 0.6983982920646667, Perplexity: 2.0105299949645996\nStep 213: Loss: 0.7998842597007751, Perplexity: 2.22528338432312\nStep 214: Loss: 0.7970062494277954, Perplexity: 2.2188880443573\nStep 215: Loss: 0.7082086205482483, Perplexity: 2.030350685119629\nStep 216: Loss: 0.5708548426628113, Perplexity: 1.7697793245315552\nStep 217: Loss: 0.5472140312194824, Perplexity: 1.728430986404419\nStep 218: Loss: 0.5131098628044128, Perplexity: 1.6704779863357544\nStep 219: Loss: 0.5453710556030273, Perplexity: 1.7252484560012817\nStep 220: Loss: 0.5053286552429199, Perplexity: 1.6575303077697754\nStep 221: Loss: 0.610556423664093, Perplexity: 1.8414558172225952\nStep 222: Loss: 0.5433598160743713, Perplexity: 1.7217820882797241\nStep 223: Loss: 0.5361123085021973, Perplexity: 1.7093485593795776\nStep 224: Loss: 0.582044780254364, Perplexity: 1.7896943092346191\nStep 225: Loss: 1.0853815078735352, Perplexity: 2.960568904876709\nStep 226: Loss: 0.5510839819908142, Perplexity: 1.7351329326629639\nStep 227: Loss: 0.6107805371284485, Perplexity: 1.84186851978302\nStep 228: Loss: 0.5788439512252808, Perplexity: 1.7839750051498413\nStep 229: Loss: 0.7888026237487793, Perplexity: 2.2007596492767334\nStep 230: Loss: 0.5771459341049194, Perplexity: 1.780948281288147\nStep 231: Loss: 0.527742326259613, Perplexity: 1.695101022720337\nStep 232: Loss: 1.7396838665008545, Perplexity: 5.69554328918457\nStep 233: Loss: 0.9610580801963806, Perplexity: 2.6144614219665527\nStep 234: Loss: 0.5026198625564575, Perplexity: 1.6530463695526123\nStep 235: Loss: 0.5623952746391296, Perplexity: 1.754870891571045\nStep 236: Loss: 0.49480658769607544, Perplexity: 1.640181064605713\nStep 237: Loss: 0.5249354839324951, Perplexity: 1.690349817276001\nStep 238: Loss: 0.5750662684440613, Perplexity: 1.7772482633590698\nStep 239: Loss: 0.5138254165649414, Perplexity: 1.6716737747192383\nStep 240: Loss: 0.499235600233078, Perplexity: 1.6474614143371582\nStep 241: Loss: 0.5879157781600952, Perplexity: 1.8002324104309082\nStep 242: Loss: 0.4569154977798462, Perplexity: 1.579195499420166\nStep 243: Loss: 0.6777945160865784, Perplexity: 1.9695292711257935\nStep 244: Loss: 0.4519171416759491, Perplexity: 1.571321725845337\nStep 245: Loss: 0.5609909892082214, Perplexity: 1.7524083852767944\nStep 246: Loss: 0.5374143719673157, Perplexity: 1.7115756273269653\nStep 247: Loss: 0.43209734559059143, Perplexity: 1.540485143661499\nStep 248: Loss: 0.5232799053192139, Perplexity: 1.6875535249710083\nStep 249: Loss: 0.5779885053634644, Perplexity: 1.78244948387146\nEpoch 1: Train Loss:  0.7241, Valid loss:  0.4266, Gap: -0.2974, Train Perp:  2.1568, Valid Perp:  1.5339\nDuration: 203.7461953163147\nStep 0: Loss: 0.4635043144226074, Perplexity: 1.5896347761154175\nStep 1: Loss: 0.5555894374847412, Perplexity: 1.7429680824279785\nStep 2: Loss: 0.5351704359054565, Perplexity: 1.707739233970642\nStep 3: Loss: 0.5904830694198608, Perplexity: 1.8048601150512695\nStep 4: Loss: 0.5066519975662231, Perplexity: 1.6597250699996948\nStep 5: Loss: 0.466696560382843, Perplexity: 1.5947173833847046\nStep 6: Loss: 0.44027838110923767, Perplexity: 1.553139567375183\nStep 7: Loss: 0.5024741291999817, Perplexity: 1.6528055667877197\nStep 8: Loss: 0.5133895874023438, Perplexity: 1.670945405960083\nStep 9: Loss: 0.828597903251648, Perplexity: 2.2901055812835693\nStep 10: Loss: 0.5545884966850281, Perplexity: 1.7412242889404297\nStep 11: Loss: 0.5089738368988037, Perplexity: 1.6635831594467163\nStep 12: Loss: 0.5694751143455505, Perplexity: 1.7673391103744507\nStep 13: Loss: 0.5287662744522095, Perplexity: 1.6968376636505127\nStep 14: Loss: 0.6231721639633179, Perplexity: 1.864834189414978\nStep 15: Loss: 0.5745673775672913, Perplexity: 1.7763619422912598\nStep 16: Loss: 0.5160962343215942, Perplexity: 1.6754741668701172\nStep 17: Loss: 0.49423840641975403, Perplexity: 1.639249324798584\nStep 18: Loss: 0.6060839295387268, Perplexity: 1.8332382440567017\nStep 19: Loss: 0.5540005564689636, Perplexity: 1.7402008771896362\nStep 20: Loss: 0.5645300149917603, Perplexity: 1.758621096611023\nStep 21: Loss: 0.4869193434715271, Perplexity: 1.6272952556610107\nStep 22: Loss: 0.5278511643409729, Perplexity: 1.6952855587005615\nStep 23: Loss: 0.5290655493736267, Perplexity: 1.697345495223999\nStep 24: Loss: 0.49106472730636597, Perplexity: 1.6340551376342773\nStep 25: Loss: 0.4964374899864197, Perplexity: 1.6428581476211548\nStep 26: Loss: 0.49022871255874634, Perplexity: 1.6326895952224731\nStep 27: Loss: 0.539872944355011, Perplexity: 1.7157889604568481\nStep 28: Loss: 0.5147961974143982, Perplexity: 1.6732975244522095\nStep 29: Loss: 0.48434847593307495, Perplexity: 1.623117208480835\nStep 30: Loss: 0.6016020178794861, Perplexity: 1.8250401020050049\nStep 31: Loss: 0.524232029914856, Perplexity: 1.6891610622406006\nStep 32: Loss: 0.5388959050178528, Perplexity: 1.7141132354736328\nStep 33: Loss: 0.5460242629051208, Perplexity: 1.726375699043274\nStep 34: Loss: 0.5435433387756348, Perplexity: 1.7220981121063232\nStep 35: Loss: 0.6356887817382812, Perplexity: 1.8883222341537476\nStep 36: Loss: 0.5083370804786682, Perplexity: 1.6625242233276367\nStep 37: Loss: 0.5692636370658875, Perplexity: 1.766965389251709\nStep 38: Loss: 0.592515230178833, Perplexity: 1.8085315227508545\nStep 39: Loss: 0.5164467692375183, Perplexity: 1.6760616302490234\nStep 40: Loss: 0.5893124938011169, Perplexity: 1.8027485609054565\nStep 41: Loss: 0.42367300391197205, Perplexity: 1.527561902999878\nStep 42: Loss: 0.3943655788898468, Perplexity: 1.483442783355713\nStep 43: Loss: 0.5431797504425049, Perplexity: 1.721471905708313\nStep 44: Loss: 0.49116843938827515, Perplexity: 1.634224534034729\nStep 45: Loss: 0.5841944217681885, Perplexity: 1.7935454845428467\nStep 46: Loss: 0.4262036383152008, Perplexity: 1.5314326286315918\nStep 47: Loss: 0.5705029964447021, Perplexity: 1.7691566944122314\nStep 48: Loss: 0.5030162930488586, Perplexity: 1.6537017822265625\nStep 49: Loss: 0.4535864591598511, Perplexity: 1.5739469528198242\nStep 50: Loss: 0.516689658164978, Perplexity: 1.676468849182129\nStep 51: Loss: 0.5335208177566528, Perplexity: 1.704924464225769\nStep 52: Loss: 0.7711784839630127, Perplexity: 2.1623129844665527\nStep 53: Loss: 0.49022620916366577, Perplexity: 1.6326855421066284\nStep 54: Loss: 0.440103679895401, Perplexity: 1.5528682470321655\nStep 55: Loss: 0.5262464880943298, Perplexity: 1.6925673484802246\nStep 56: Loss: 0.40854692459106445, Perplexity: 1.5046299695968628\nStep 57: Loss: 0.4734971821308136, Perplexity: 1.6055995225906372\nStep 58: Loss: 0.4669118821620941, Perplexity: 1.5950608253479004\nStep 59: Loss: 0.6091185212135315, Perplexity: 1.838809847831726\nStep 60: Loss: 0.5178449749946594, Perplexity: 1.678406834602356\nStep 61: Loss: 0.5226432681083679, Perplexity: 1.6864795684814453\nStep 62: Loss: 0.6517444849014282, Perplexity: 1.9188854694366455\nStep 63: Loss: 1.2163046598434448, Perplexity: 3.3746941089630127\nStep 64: Loss: 0.6407324075698853, Perplexity: 1.8978703022003174\nStep 65: Loss: 0.49819785356521606, Perplexity: 1.6457526683807373\nStep 66: Loss: 0.5616276860237122, Perplexity: 1.7535244226455688\nStep 67: Loss: 0.4492938220500946, Perplexity: 1.5672050714492798\nStep 68: Loss: 0.5479390025138855, Perplexity: 1.7296844720840454\nStep 69: Loss: 0.502426266670227, Perplexity: 1.6527262926101685\nStep 70: Loss: 0.5331169962882996, Perplexity: 1.7042361497879028\nStep 71: Loss: 0.4751147925853729, Perplexity: 1.6081987619400024\nStep 72: Loss: 0.5399816632270813, Perplexity: 1.715975284576416\nStep 73: Loss: 0.47000250220298767, Perplexity: 1.599998116493225\nStep 74: Loss: 0.7529727220535278, Perplexity: 2.123302698135376\nStep 75: Loss: 0.5234057307243347, Perplexity: 1.6877659559249878\nStep 76: Loss: 0.55136638879776, Perplexity: 1.7356230020523071\nStep 77: Loss: 0.954069197177887, Perplexity: 2.596252918243408\nStep 78: Loss: 0.44730231165885925, Perplexity: 1.5640870332717896\nStep 79: Loss: 0.5719289183616638, Perplexity: 1.7716811895370483\nStep 80: Loss: 0.6296740174293518, Perplexity: 1.8769986629486084\nStep 81: Loss: 1.123234748840332, Perplexity: 3.074784278869629\nStep 82: Loss: 0.3661862015724182, Perplexity: 1.4422237873077393\nStep 83: Loss: 1.0476211309432983, Perplexity: 2.850861072540283\nStep 84: Loss: 0.6537273526191711, Perplexity: 1.9226940870285034\nStep 85: Loss: 0.45890992879867554, Perplexity: 1.5823482275009155\nStep 86: Loss: 0.4528501033782959, Perplexity: 1.5727884769439697\nStep 87: Loss: 0.53682541847229, Perplexity: 1.7105679512023926\nStep 88: Loss: 0.5694682002067566, Perplexity: 1.7673269510269165\nStep 89: Loss: 0.47372788190841675, Perplexity: 1.605970025062561\nStep 90: Loss: 0.4217599034309387, Perplexity: 1.5246424674987793\nStep 91: Loss: 0.5084149241447449, Perplexity: 1.6626535654067993\nStep 92: Loss: 0.7552029490470886, Perplexity: 2.1280434131622314\nStep 93: Loss: 0.3868349492549896, Perplexity: 1.4723135232925415\nStep 94: Loss: 0.5304548144340515, Perplexity: 1.6997052431106567\nStep 95: Loss: 0.5022750496864319, Perplexity: 1.65247642993927\nStep 96: Loss: 0.4583875238895416, Perplexity: 1.5815218687057495\nStep 97: Loss: 0.5263187885284424, Perplexity: 1.6926897764205933\nStep 98: Loss: 0.4358062446117401, Perplexity: 1.5462090969085693\nStep 99: Loss: 0.3888806104660034, Perplexity: 1.4753284454345703\nStep 100: Loss: 0.550439715385437, Perplexity: 1.7340153455734253\nStep 101: Loss: 0.5149446129798889, Perplexity: 1.6735458374023438\nStep 102: Loss: 0.5981833338737488, Perplexity: 1.8188116550445557\nStep 103: Loss: 0.5457594990730286, Perplexity: 1.7259187698364258\nStep 104: Loss: 0.4801199734210968, Perplexity: 1.616268277168274\nStep 105: Loss: 0.7195433378219604, Perplexity: 2.053495168685913\nStep 106: Loss: 0.5255229473114014, Perplexity: 1.691343069076538\nStep 107: Loss: 0.48337048292160034, Perplexity: 1.621530532836914\nStep 108: Loss: 0.46999868750572205, Perplexity: 1.599992036819458\nStep 109: Loss: 0.4792262613773346, Perplexity: 1.6148244142532349\nStep 110: Loss: 0.4326138198375702, Perplexity: 1.5412808656692505\nStep 111: Loss: 1.2000190019607544, Perplexity: 3.3201799392700195\nStep 112: Loss: 0.46281182765960693, Perplexity: 1.5885343551635742\nStep 113: Loss: 0.44485899806022644, Perplexity: 1.5602701902389526\nStep 114: Loss: 0.5489891767501831, Perplexity: 1.7315019369125366\nStep 115: Loss: 0.5156770944595337, Perplexity: 1.6747721433639526\nStep 116: Loss: 0.518462061882019, Perplexity: 1.6794427633285522\nStep 117: Loss: 0.4351913630962372, Perplexity: 1.5452587604522705\nStep 118: Loss: 0.4627188742160797, Perplexity: 1.5883866548538208\nStep 119: Loss: 0.4375860393047333, Perplexity: 1.5489635467529297\nStep 120: Loss: 0.41779085993766785, Perplexity: 1.5186030864715576\nStep 121: Loss: 0.4679751694202423, Perplexity: 1.5967577695846558\nStep 122: Loss: 0.49530431628227234, Perplexity: 1.6409975290298462\nStep 123: Loss: 0.45422491431236267, Perplexity: 1.574952244758606\nStep 124: Loss: 0.5178526043891907, Perplexity: 1.678419589996338\nStep 125: Loss: 0.49734923243522644, Perplexity: 1.6443567276000977\nStep 126: Loss: 0.5400959849357605, Perplexity: 1.7161716222763062\nStep 127: Loss: 0.559777557849884, Perplexity: 1.750283122062683\nStep 128: Loss: 0.42243629693984985, Perplexity: 1.5256741046905518\nStep 129: Loss: 0.48245033621788025, Perplexity: 1.6200392246246338\nStep 130: Loss: 0.5146036148071289, Perplexity: 1.6729751825332642\nStep 131: Loss: 0.4854123890399933, Perplexity: 1.624845027923584\nStep 132: Loss: 0.5186973810195923, Perplexity: 1.6798380613327026\nStep 133: Loss: 0.4572194516658783, Perplexity: 1.5796754360198975\nStep 134: Loss: 0.4492581784725189, Perplexity: 1.5671491622924805\nStep 135: Loss: 0.5049510598182678, Perplexity: 1.6569044589996338\nStep 136: Loss: 0.5383835434913635, Perplexity: 1.7132352590560913\nStep 137: Loss: 1.7474030256271362, Perplexity: 5.739677906036377\nStep 138: Loss: 0.5446766018867493, Perplexity: 1.724050760269165\nStep 139: Loss: 0.5101356506347656, Perplexity: 1.6655170917510986\nStep 140: Loss: 0.4722941815853119, Perplexity: 1.6036691665649414\nStep 141: Loss: 0.515982449054718, Perplexity: 1.6752835512161255\nStep 142: Loss: 0.4969620108604431, Perplexity: 1.6437201499938965\nStep 143: Loss: 0.4605519771575928, Perplexity: 1.5849485397338867\nStep 144: Loss: 0.4187443256378174, Perplexity: 1.5200517177581787\nStep 145: Loss: 0.5151459574699402, Perplexity: 1.6738828420639038\nStep 146: Loss: 0.41305994987487793, Perplexity: 1.511435627937317\nStep 147: Loss: 0.5203698873519897, Perplexity: 1.6826499700546265\nStep 148: Loss: 0.45372283458709717, Perplexity: 1.5741616487503052\nStep 149: Loss: 0.430052250623703, Perplexity: 1.5373377799987793\nStep 150: Loss: 0.6823603510856628, Perplexity: 1.9785422086715698\nStep 151: Loss: 0.5005618333816528, Perplexity: 1.649647831916809\nStep 152: Loss: 0.5387377738952637, Perplexity: 1.7138422727584839\nStep 153: Loss: 0.5015650987625122, Perplexity: 1.6513036489486694\nStep 154: Loss: 0.5328211188316345, Perplexity: 1.703731894493103\nStep 155: Loss: 0.507425844669342, Perplexity: 1.6610099077224731\nStep 156: Loss: 0.47553205490112305, Perplexity: 1.6088699102401733\nStep 157: Loss: 0.4297563135623932, Perplexity: 1.536882996559143\nStep 158: Loss: 0.6090387105941772, Perplexity: 1.838663101196289\nStep 159: Loss: 0.5251260995864868, Perplexity: 1.6906719207763672\nStep 160: Loss: 0.5227859616279602, Perplexity: 1.6867202520370483\nStep 161: Loss: 0.35268327593803406, Perplexity: 1.4228804111480713\nStep 162: Loss: 0.45150086283683777, Perplexity: 1.5706677436828613\nStep 163: Loss: 0.6093955636024475, Perplexity: 1.8393193483352661\nStep 164: Loss: 0.5616977214813232, Perplexity: 1.7536472082138062\nStep 165: Loss: 0.4472798705101013, Perplexity: 1.5640519857406616\nStep 166: Loss: 0.43511295318603516, Perplexity: 1.5451375246047974\nStep 167: Loss: 0.5238648056983948, Perplexity: 1.6885409355163574\nStep 168: Loss: 0.5358339548110962, Perplexity: 1.7088727951049805\nStep 169: Loss: 0.5701342821121216, Perplexity: 1.7685045003890991\nStep 170: Loss: 0.41225820779800415, Perplexity: 1.5102243423461914\nStep 171: Loss: 0.4352833330631256, Perplexity: 1.545400857925415\nStep 172: Loss: 0.5459126234054565, Perplexity: 1.7261830568313599\nStep 173: Loss: 0.37742286920547485, Perplexity: 1.4585210084915161\nStep 174: Loss: 0.3905414342880249, Perplexity: 1.4777805805206299\nStep 175: Loss: 0.4801616966724396, Perplexity: 1.6163357496261597\nStep 176: Loss: 0.5515764951705933, Perplexity: 1.7359875440597534\nStep 177: Loss: 0.5116633176803589, Perplexity: 1.6680634021759033\nStep 178: Loss: 0.4492095410823822, Perplexity: 1.5670729875564575\nStep 179: Loss: 0.5369421243667603, Perplexity: 1.7107675075531006\nStep 180: Loss: 0.43581879138946533, Perplexity: 1.5462285280227661\nStep 181: Loss: 0.4777137041091919, Perplexity: 1.6123838424682617\nStep 182: Loss: 0.5030888915061951, Perplexity: 1.6538219451904297\nStep 183: Loss: 0.3841179311275482, Perplexity: 1.4683185815811157\nStep 184: Loss: 0.43076446652412415, Perplexity: 1.5384331941604614\nStep 185: Loss: 0.5093933939933777, Perplexity: 1.6642812490463257\nStep 186: Loss: 0.6963156461715698, Perplexity: 2.0063469409942627\nStep 187: Loss: 0.422504723072052, Perplexity: 1.5257784128189087\nStep 188: Loss: 0.49554431438446045, Perplexity: 1.6413915157318115\nStep 189: Loss: 0.3663457930088043, Perplexity: 1.4424539804458618\nStep 190: Loss: 0.41659244894981384, Perplexity: 1.5167841911315918\nStep 191: Loss: 0.48830491304397583, Perplexity: 1.629551649093628\nStep 192: Loss: 0.45735862851142883, Perplexity: 1.5798953771591187\nStep 193: Loss: 0.3877743184566498, Perplexity: 1.4736971855163574\nStep 194: Loss: 0.4085160493850708, Perplexity: 1.504583477973938\nStep 195: Loss: 0.457711398601532, Perplexity: 1.580452799797058\nStep 196: Loss: 0.3953463137149811, Perplexity: 1.484898328781128\nStep 197: Loss: 0.5308282375335693, Perplexity: 1.7003400325775146\nStep 198: Loss: 1.2947124242782593, Perplexity: 3.6499462127685547\nStep 199: Loss: 0.41289812326431274, Perplexity: 1.5111911296844482\nStep 200: Loss: 0.3840559720993042, Perplexity: 1.4682276248931885\nStep 201: Loss: 0.4561874270439148, Perplexity: 1.578046202659607\nStep 202: Loss: 0.4317861795425415, Perplexity: 1.5400058031082153\nStep 203: Loss: 0.951536238193512, Perplexity: 2.5896852016448975\nStep 204: Loss: 0.5595541596412659, Perplexity: 1.749892234802246\nStep 205: Loss: 0.5281265377998352, Perplexity: 1.6957523822784424\nStep 206: Loss: 0.5815825462341309, Perplexity: 1.7888672351837158\nStep 207: Loss: 0.425140380859375, Perplexity: 1.5298051834106445\nStep 208: Loss: 0.5426696538925171, Perplexity: 1.720594048500061\nStep 209: Loss: 0.477100133895874, Perplexity: 1.6113947629928589\nStep 210: Loss: 0.5808513760566711, Perplexity: 1.7875597476959229\nStep 211: Loss: 0.4476083517074585, Perplexity: 1.5645657777786255\nStep 212: Loss: 0.44816768169403076, Perplexity: 1.5654411315917969\nStep 213: Loss: 0.5154592394828796, Perplexity: 1.6744072437286377\nStep 214: Loss: 0.4729551076889038, Perplexity: 1.6047292947769165\nStep 215: Loss: 0.4221315383911133, Perplexity: 1.5252090692520142\nStep 216: Loss: 0.4943215847015381, Perplexity: 1.63938570022583\nStep 217: Loss: 0.49792689085006714, Perplexity: 1.6453068256378174\nStep 218: Loss: 0.42952030897140503, Perplexity: 1.5365203619003296\nStep 219: Loss: 0.4615296721458435, Perplexity: 1.5864989757537842\nStep 220: Loss: 0.5037459135055542, Perplexity: 1.6549087762832642\nStep 221: Loss: 0.3990729749202728, Perplexity: 1.4904423952102661\nStep 222: Loss: 0.40288031101226807, Perplexity: 1.4961278438568115\nStep 223: Loss: 0.3831702470779419, Perplexity: 1.4669277667999268\nStep 224: Loss: 0.5613978505134583, Perplexity: 1.7531214952468872\nStep 225: Loss: 0.4301179051399231, Perplexity: 1.5374388694763184\nStep 226: Loss: 0.45528849959373474, Perplexity: 1.5766282081604004\nStep 227: Loss: 0.40591001510620117, Perplexity: 1.5006674528121948\nStep 228: Loss: 0.4429861009120941, Perplexity: 1.5573506355285645\nStep 229: Loss: 0.554966151714325, Perplexity: 1.741882085800171\nStep 230: Loss: 0.5637241005897522, Perplexity: 1.757204294204712\nStep 231: Loss: 0.47036322951316833, Perplexity: 1.6005754470825195\nStep 232: Loss: 0.47195079922676086, Perplexity: 1.6031185388565063\nStep 233: Loss: 0.5115970969200134, Perplexity: 1.6679530143737793\nStep 234: Loss: 0.8662552237510681, Perplexity: 2.3779890537261963\nStep 235: Loss: 0.4428401589393616, Perplexity: 1.5571234226226807\nStep 236: Loss: 0.44308096170425415, Perplexity: 1.5574984550476074\nStep 237: Loss: 0.3483448326587677, Perplexity: 1.4167207479476929\nStep 238: Loss: 0.484396755695343, Perplexity: 1.6231956481933594\nStep 239: Loss: 0.4318893253803253, Perplexity: 1.5401647090911865\nStep 240: Loss: 0.3987429440021515, Perplexity: 1.4899506568908691\nStep 241: Loss: 0.3918023109436035, Perplexity: 1.4796451330184937\nStep 242: Loss: 0.5226092338562012, Perplexity: 1.6864222288131714\nStep 243: Loss: 0.46352317929267883, Perplexity: 1.5896648168563843\nStep 244: Loss: 0.48569098114967346, Perplexity: 1.6252977848052979\nStep 245: Loss: 0.4173966646194458, Perplexity: 1.5180045366287231\nStep 246: Loss: 0.45210152864456177, Perplexity: 1.5716115236282349\nStep 247: Loss: 0.46608349680900574, Perplexity: 1.5937401056289673\nStep 248: Loss: 0.4570780396461487, Perplexity: 1.5794521570205688\nStep 249: Loss: 0.3374321758747101, Perplexity: 1.4013445377349854\nEpoch 1: Train Loss:  0.5228, Valid loss:  0.3569, Gap: -0.1659, Train Perp:  1.7109, Valid Perp:  1.4303\nDuration: 203.02145719528198\nStep 0: Loss: 0.39925438165664673, Perplexity: 1.4907127618789673\nStep 1: Loss: 0.48121073842048645, Perplexity: 1.6180322170257568\nStep 2: Loss: 0.41144511103630066, Perplexity: 1.508996844291687\nStep 3: Loss: 0.4865451157093048, Perplexity: 1.6266865730285645\nStep 4: Loss: 0.46824267506599426, Perplexity: 1.5971850156784058\nStep 5: Loss: 0.4907463788986206, Perplexity: 1.6335349082946777\nStep 6: Loss: 0.47964322566986084, Perplexity: 1.6154978275299072\nStep 7: Loss: 0.3796081840991974, Perplexity: 1.4617117643356323\nStep 8: Loss: 0.47597259283065796, Perplexity: 1.6095789670944214\nStep 9: Loss: 0.41851499676704407, Perplexity: 1.5197030305862427\nStep 10: Loss: 0.45284950733184814, Perplexity: 1.5727874040603638\nStep 11: Loss: 0.4985639154911041, Perplexity: 1.646355152130127\nStep 12: Loss: 0.420223206281662, Perplexity: 1.5223013162612915\nStep 13: Loss: 0.42166364192962646, Perplexity: 1.5244957208633423\nStep 14: Loss: 0.39454734325408936, Perplexity: 1.4837124347686768\nStep 15: Loss: 0.42501741647720337, Perplexity: 1.5296170711517334\nStep 16: Loss: 0.4480164349079132, Perplexity: 1.565204381942749\nStep 17: Loss: 0.4818713068962097, Perplexity: 1.6191014051437378\nStep 18: Loss: 0.47612231969833374, Perplexity: 1.609820008277893\nStep 19: Loss: 0.4643903374671936, Perplexity: 1.5910439491271973\nStep 20: Loss: 0.4800482392311096, Perplexity: 1.6161524057388306\nStep 21: Loss: 0.3889339864253998, Perplexity: 1.4754072427749634\nStep 22: Loss: 0.4794465899467468, Perplexity: 1.6151803731918335\nStep 23: Loss: 0.4891543388366699, Perplexity: 1.6309363842010498\nStep 24: Loss: 0.3868451714515686, Perplexity: 1.4723286628723145\nStep 25: Loss: 0.45753785967826843, Perplexity: 1.5801786184310913\nStep 26: Loss: 0.4437510371208191, Perplexity: 1.5585424900054932\nStep 27: Loss: 0.5117883682250977, Perplexity: 1.6682718992233276\nStep 28: Loss: 0.43093401193618774, Perplexity: 1.5386940240859985\nStep 29: Loss: 0.4614434838294983, Perplexity: 1.5863621234893799\nStep 30: Loss: 0.4390014410018921, Perplexity: 1.5511574745178223\nStep 31: Loss: 0.46200430393218994, Perplexity: 1.5872522592544556\nStep 32: Loss: 0.3961833715438843, Perplexity: 1.4861418008804321\nStep 33: Loss: 0.5550339818000793, Perplexity: 1.7420002222061157\nStep 34: Loss: 0.4098697006702423, Perplexity: 1.5066214799880981\nStep 35: Loss: 0.4160368740558624, Perplexity: 1.5159417390823364\nStep 36: Loss: 0.3876531720161438, Perplexity: 1.4735186100006104\nStep 37: Loss: 0.41323745250701904, Perplexity: 1.5117038488388062\nStep 38: Loss: 0.4020955562591553, Perplexity: 1.494954228401184\nStep 39: Loss: 0.5054327249526978, Perplexity: 1.6577026844024658\nStep 40: Loss: 0.49475476145744324, Perplexity: 1.6400959491729736\nStep 41: Loss: 0.44679510593414307, Perplexity: 1.5632939338684082\nStep 42: Loss: 0.5000694394111633, Perplexity: 1.6488357782363892\nStep 43: Loss: 0.4543609917163849, Perplexity: 1.5751665830612183\nStep 44: Loss: 0.4513457417488098, Perplexity: 1.570424199104309\nStep 45: Loss: 0.43840131163597107, Perplexity: 1.5502268075942993\nStep 46: Loss: 0.45458218455314636, Perplexity: 1.5755150318145752\nStep 47: Loss: 0.40330591797828674, Perplexity: 1.4967646598815918\nStep 48: Loss: 0.4698292315006256, Perplexity: 1.5997209548950195\nStep 49: Loss: 0.3548075556755066, Perplexity: 1.4259063005447388\nStep 50: Loss: 0.3353993594646454, Perplexity: 1.398498773574829\nStep 51: Loss: 0.4217495918273926, Perplexity: 1.5246267318725586\nStep 52: Loss: 0.4018040597438812, Perplexity: 1.4945183992385864\nStep 53: Loss: 0.41741859912872314, Perplexity: 1.5180377960205078\nStep 54: Loss: 0.47579705715179443, Perplexity: 1.6092963218688965\nStep 55: Loss: 0.49803128838539124, Perplexity: 1.6454787254333496\nStep 56: Loss: 0.48980364203453064, Perplexity: 1.631995677947998\nStep 57: Loss: 0.48468017578125, Perplexity: 1.6236557960510254\nStep 58: Loss: 0.5214619040489197, Perplexity: 1.6844884157180786\nStep 59: Loss: 0.3843995928764343, Perplexity: 1.4687323570251465\nStep 60: Loss: 0.48825106024742126, Perplexity: 1.629463791847229\nStep 61: Loss: 0.42762643098831177, Perplexity: 1.5336130857467651\nStep 62: Loss: 0.40419235825538635, Perplexity: 1.4980920553207397\nStep 63: Loss: 0.4555041193962097, Perplexity: 1.5769681930541992\nStep 64: Loss: 0.36828774213790894, Perplexity: 1.4452579021453857\nStep 65: Loss: 0.4420982003211975, Perplexity: 1.5559685230255127\nStep 66: Loss: 0.3871619403362274, Perplexity: 1.472795009613037\nStep 67: Loss: 0.4837509095668793, Perplexity: 1.622147560119629\nStep 68: Loss: 0.46399304270744324, Perplexity: 1.590411901473999\nStep 69: Loss: 0.4995020925998688, Perplexity: 1.6479004621505737\nStep 70: Loss: 0.40678247809410095, Perplexity: 1.5019773244857788\nStep 71: Loss: 0.47536730766296387, Perplexity: 1.608604907989502\nStep 72: Loss: 0.5126853585243225, Perplexity: 1.669769048690796\nStep 73: Loss: 0.48247435688972473, Perplexity: 1.6200780868530273\nStep 74: Loss: 0.4126898944377899, Perplexity: 1.5108764171600342\nStep 75: Loss: 0.48458540439605713, Perplexity: 1.6235018968582153\nStep 76: Loss: 0.39750784635543823, Perplexity: 1.4881113767623901\nStep 77: Loss: 0.3835810124874115, Perplexity: 1.4675304889678955\nStep 78: Loss: 0.404880553483963, Perplexity: 1.4991233348846436\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2317715707.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_decays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         train_loss, val_loss, gap, train_perp, val_perp  = overfit_experiment(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msubset_train_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/378209277.py\u001b[0m in \u001b[0;36moverfit_experiment\u001b[0;34m(model, train_dl, valid_dl, train_len, valid_len, learning_rate, epochs, dropout_rate, weight_decay)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:59:30.660788Z","iopub.execute_input":"2025-05-04T17:59:30.661348Z","iopub.status.idle":"2025-05-04T17:59:30.670449Z","shell.execute_reply.started":"2025-05-04T17:59:30.661324Z","shell.execute_reply":"2025-05-04T17:59:30.669800Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"   dropout  weight_decay  train_loss  val_loss  overfit_gap\n0      0.1          0.00    3.664187  0.920810    -2.743378\n1      0.1          0.01    0.667940  0.398948    -0.268992\n2      0.2          0.00    0.499912  0.334794    -0.165117\n3      0.2          0.01    0.419839  0.294916    -0.124923\n4      0.3          0.00    0.377740  0.279421    -0.098318\n5      0.3          0.01    0.359941  0.272403    -0.087538","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dropout</th>\n      <th>weight_decay</th>\n      <th>train_loss</th>\n      <th>val_loss</th>\n      <th>overfit_gap</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.1</td>\n      <td>0.00</td>\n      <td>3.664187</td>\n      <td>0.920810</td>\n      <td>-2.743378</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>0.667940</td>\n      <td>0.398948</td>\n      <td>-0.268992</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.2</td>\n      <td>0.00</td>\n      <td>0.499912</td>\n      <td>0.334794</td>\n      <td>-0.165117</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.2</td>\n      <td>0.01</td>\n      <td>0.419839</td>\n      <td>0.294916</td>\n      <td>-0.124923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.3</td>\n      <td>0.00</td>\n      <td>0.377740</td>\n      <td>0.279421</td>\n      <td>-0.098318</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.3</td>\n      <td>0.01</td>\n      <td>0.359941</td>\n      <td>0.272403</td>\n      <td>-0.087538</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":63}]}